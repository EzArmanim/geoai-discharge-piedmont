{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb07782-945d-4a27-99cf-50a3a5cf86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy scikit-learn matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285ba4a-6010-4c47-9470-341c22f815fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79410d4-4985-43f4-bf43-e3d81a779a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning the model with 5 configs of XGBoost algorithm with different depth & learning rate combos.\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "RS_FILENAME  = \"RS_monthly.csv\"         # Remote sensing table (from GEE)\n",
    "Q_FILENAME   = \"discharge_monthly.csv\"  # Monthly discharge table\n",
    "OUT_DIR      = \"outputs\"\n",
    "CUTOFF_DATE  = \"2018-01-01\"             # Train < cutoff, Test >= cutoff\n",
    "MIN_TEST_ROWS_PER_STATION = 6\n",
    "\n",
    "# RMSE helper (future-proof for old/new sklearn versions)\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error as rmse_func\n",
    "except ImportError:\n",
    "    def rmse_func(y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "# Hydrology metric: Nash–Sutcliffe Efficiency\n",
    "def nse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    return 1 - ((y_true - y_pred)**2).sum() / ((y_true - y_true.mean())**2).sum()\n",
    "\n",
    "# -------------------------\n",
    "# Utility functions\n",
    "# -------------------------\n",
    "def std_name(s):\n",
    "    \"\"\"Normalize station names (fix whitespace & known typos).\"\"\"\n",
    "    return \" \".join(str(s).split()).replace(\"Soucherres\", \"Soucheres\")\n",
    "\n",
    "def find_col(candidates, cols):\n",
    "    \"\"\"Find a column name in `cols` matching one of `candidates` (exact or prefix/suffix).\"\"\"\n",
    "    cols = list(cols)\n",
    "    for c in candidates:\n",
    "        if c in cols: return c\n",
    "    for col in cols:\n",
    "        for c in candidates:\n",
    "            if col.lower().startswith(c.lower()) or col.lower().endswith(c.lower()):\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# -------------------------\n",
    "# Load Remote Sensing data\n",
    "# -------------------------\n",
    "rs = pd.read_csv(RS_FILENAME)\n",
    "\n",
    "# Normalize date column (sometimes called `date_str`)\n",
    "if \"date\" not in rs.columns and \"date_str\" in rs.columns:\n",
    "    rs = rs.rename(columns={\"date_str\": \"date\"})\n",
    "rs[\"date\"] = pd.to_datetime(rs[\"date\"], errors=\"coerce\")\n",
    "\n",
    "# Find precipitation and NDVI columns (names may vary from GEE export)\n",
    "p_col = find_col([\"P_mm\",\"precip\",\"rain_mm\",\"precip_mm\"], rs.columns)\n",
    "n_col = find_col([\"NDVI\"], rs.columns)\n",
    "rs = rs[[\"station_id\",\"date\",p_col,n_col]].rename(columns={p_col:\"P_mm\", n_col:\"NDVI\"})\n",
    "\n",
    "# -------------------------\n",
    "# Load Discharge (Q) data\n",
    "# -------------------------\n",
    "q = pd.read_csv(Q_FILENAME, parse_dates=[\"date\"])[[\"station_id\",\"date\",\"Q_m3s\"]]\n",
    "q[\"station_id\"]  = q[\"station_id\"].map(std_name)\n",
    "rs[\"station_id\"] = rs[\"station_id\"].map(std_name)\n",
    "\n",
    "# -------------------------\n",
    "# Merge datasets (keep only rows where both Q and RS exist)\n",
    "# -------------------------\n",
    "df = (q.merge(rs, on=[\"station_id\",\"date\"], how=\"inner\")\n",
    "        .sort_values([\"station_id\",\"date\"])\n",
    "        .dropna(subset=[\"Q_m3s\",\"P_mm\",\"NDVI\"])\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# -------------------------\n",
    "# Add lags & rolling features (catchment memory)\n",
    "# -------------------------\n",
    "def add_lags_rollups(g):\n",
    "    # Precipitation memory\n",
    "    g[\"P_mm_lag1\"] = g[\"P_mm\"].shift(1)\n",
    "    g[\"P_mm_lag2\"] = g[\"P_mm\"].shift(2)\n",
    "    g[\"P_mm_lag3\"] = g[\"P_mm\"].shift(3)\n",
    "    g[\"P_3mo_sum\"] = g[\"P_mm\"].rolling(3).sum()\n",
    "    g[\"P_6mo_sum\"] = g[\"P_mm\"].rolling(6).sum()\n",
    "    # Vegetation / wetness memory\n",
    "    g[\"NDVI_lag1\"]   = g[\"NDVI\"].shift(1)\n",
    "    g[\"NDVI_lag2\"]   = g[\"NDVI\"].shift(2)\n",
    "    g[\"NDVI_3mo_mn\"] = g[\"NDVI\"].rolling(3).mean()\n",
    "    return g\n",
    "\n",
    "df = df.groupby(\"station_id\", group_keys=False).apply(add_lags_rollups)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# -------------------------\n",
    "# Add seasonality (smooth month encoding)\n",
    "# -------------------------\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*(df[\"month\"]/12))\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*(df[\"month\"]/12))\n",
    "\n",
    "# -------------------------\n",
    "# Train/Test split (time-aware)\n",
    "# -------------------------\n",
    "cutoff = pd.Timestamp(CUTOFF_DATE)\n",
    "train, test = df[df[\"date\"] < cutoff], df[df[\"date\"] >= cutoff]\n",
    "\n",
    "features = [\n",
    "    \"P_mm\",\"NDVI\",\"month_sin\",\"month_cos\",\n",
    "    \"P_mm_lag1\",\"P_mm_lag2\",\"P_mm_lag3\",\n",
    "    \"P_3mo_sum\",\"P_6mo_sum\",\n",
    "    \"NDVI_lag1\",\"NDVI_lag2\",\"NDVI_3mo_mn\"\n",
    "]\n",
    "\n",
    "X_tr, y_tr = train[features], train[\"Q_m3s\"]\n",
    "X_te, y_te = test[features],  test[\"Q_m3s\"]\n",
    "\n",
    "# -------------------------\n",
    "# Parameter tuning sweep\n",
    "# -------------------------\n",
    "param_grid = [\n",
    "    # Lower depth = simpler trees (less overfit), higher = more complex\n",
    "    {\"max_depth\":4, \"learning_rate\":0.05, \"subsample\":0.8, \"colsample_bytree\":0.8},\n",
    "    {\"max_depth\":6, \"learning_rate\":0.05, \"subsample\":0.8, \"colsample_bytree\":0.8},\n",
    "    {\"max_depth\":8, \"learning_rate\":0.05, \"subsample\":0.8, \"colsample_bytree\":0.8},\n",
    "    # Try smaller learning rate with more stochastic sampling\n",
    "    {\"max_depth\":6, \"learning_rate\":0.03, \"subsample\":0.9, \"colsample_bytree\":0.9},\n",
    "    # Try faster learning rate (may overfit but worth testing)\n",
    "    {\"max_depth\":6, \"learning_rate\":0.1,  \"subsample\":0.8, \"colsample_bytree\":0.8},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, params in enumerate(param_grid, 1):\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=800,      # number of trees (enough for convergence)\n",
    "        reg_lambda=1.0,        # L2 regularization\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",    # fast CPU mode\n",
    "        **params               # inject params from the grid\n",
    "    )\n",
    "    # Train and evaluate\n",
    "    model.fit(X_tr, y_tr)\n",
    "    pred = model.predict(X_te)\n",
    "    results.append({\n",
    "        \"config\": params,\n",
    "        \"R2\": r2_score(y_te, pred),\n",
    "        \"RMSE\": rmse_func(y_te, pred),\n",
    "        \"NSE\": nse(y_te, pred)\n",
    "    })\n",
    "    print(f\"Config {i}: {params} → NSE={results[-1]['NSE']:.3f}, R2={results[-1]['R2']:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Save leaderboard\n",
    "# -------------------------\n",
    "resdf = pd.DataFrame(results).sort_values(\"NSE\", ascending=False)\n",
    "Path(OUT_DIR).mkdir(exist_ok=True)\n",
    "resdf.to_csv(Path(OUT_DIR)/\"xgb_tuning_results.csv\", index=False)\n",
    "\n",
    "print(\"\\nBest config:\\n\", resdf.head(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd1a5e-cac3-4e76-8bd2-77307d6ba876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename=\"outputs/obs_vs_pred.png\"))\n",
    "display(Image(filename=\"outputs/feature_importance.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8558c833-20aa-4f18-9679-85a027393a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model wit the best parameter from 5 configs of XGBoost with different depth & learning rate combos.\n",
    "# Finalize best XGB config and save outputs\n",
    "import pandas as pd, numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# ---------- paths ----------\n",
    "RS_FILENAME  = \"RS_monthly.csv\"\n",
    "Q_FILENAME   = \"discharge_monthly.csv\"\n",
    "OUT_DIR      = Path(\"outputs\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "CUTOFF_DATE  = \"2018-01-01\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "try:\n",
    "    from sklearn.metrics import root_mean_squared_error as rmse_func\n",
    "except ImportError:\n",
    "    def rmse_func(y_true, y_pred):\n",
    "        return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def nse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true); y_pred = np.asarray(y_pred)\n",
    "    return 1 - ((y_true - y_pred)**2).sum() / ((y_true - y_true.mean())**2).sum()\n",
    "\n",
    "def std_name(s): return \" \".join(str(s).split()).replace(\"Soucherres\", \"Soucheres\")\n",
    "def find_col(cands, cols):\n",
    "    cols = list(cols)\n",
    "    for c in cands:\n",
    "        if c in cols: return c\n",
    "    for col in cols:\n",
    "        for c in cands:\n",
    "            if col.lower().startswith(c.lower()) or col.lower().endswith(c.lower()):\n",
    "                return col\n",
    "    return None\n",
    "\n",
    "# ---------- load RS ----------\n",
    "rs = pd.read_csv(RS_FILENAME)\n",
    "if \"date\" not in rs.columns and \"date_str\" in rs.columns:\n",
    "    rs = rs.rename(columns={\"date_str\": \"date\"})\n",
    "rs[\"date\"] = pd.to_datetime(rs[\"date\"], errors=\"coerce\")\n",
    "p_col = find_col([\"P_mm\",\"precip\",\"rain_mm\",\"precip_mm\"], rs.columns)\n",
    "n_col = find_col([\"NDVI\"], rs.columns)\n",
    "rs = rs[[\"station_id\",\"date\",p_col,n_col]].rename(columns={p_col:\"P_mm\", n_col:\"NDVI\"})\n",
    "\n",
    "# ---------- load Q ----------\n",
    "q = pd.read_csv(Q_FILENAME, parse_dates=[\"date\"])[[\"station_id\",\"date\",\"Q_m3s\"]]\n",
    "q[\"station_id\"]  = q[\"station_id\"].map(std_name)\n",
    "rs[\"station_id\"] = rs[\"station_id\"].map(std_name)\n",
    "\n",
    "# ---------- merge ----------\n",
    "df = (q.merge(rs, on=[\"station_id\",\"date\"], how=\"inner\")\n",
    "        .sort_values([\"station_id\",\"date\"])\n",
    "        .dropna(subset=[\"Q_m3s\",\"P_mm\",\"NDVI\"])\n",
    "        .reset_index(drop=True))\n",
    "\n",
    "# ---------- lags/rollups ----------\n",
    "def add_lags_rollups(g):\n",
    "    g[\"P_mm_lag1\"] = g[\"P_mm\"].shift(1)\n",
    "    g[\"P_mm_lag2\"] = g[\"P_mm\"].shift(2)\n",
    "    g[\"P_mm_lag3\"] = g[\"P_mm\"].shift(3)\n",
    "    g[\"P_3mo_sum\"] = g[\"P_mm\"].rolling(3).sum()\n",
    "    g[\"P_6mo_sum\"] = g[\"P_mm\"].rolling(6).sum()\n",
    "    g[\"NDVI_lag1\"]   = g[\"NDVI\"].shift(1)\n",
    "    g[\"NDVI_lag2\"]   = g[\"NDVI\"].shift(2)\n",
    "    g[\"NDVI_3mo_mn\"] = g[\"NDVI\"].rolling(3).mean()\n",
    "    return g\n",
    "\n",
    "# if your pandas >=2.1 you can add include_groups=False\n",
    "df = df.groupby(\"station_id\", group_keys=False).apply(add_lags_rollups)\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "# ---------- seasonality ----------\n",
    "df[\"month\"] = df[\"date\"].dt.month\n",
    "df[\"month_sin\"] = np.sin(2*np.pi*(df[\"month\"]/12))\n",
    "df[\"month_cos\"] = np.cos(2*np.pi*(df[\"month\"]/12))\n",
    "\n",
    "# ---------- split ----------\n",
    "cutoff = pd.Timestamp(CUTOFF_DATE)\n",
    "train, test = df[df[\"date\"] < cutoff], df[df[\"date\"] >= cutoff]\n",
    "\n",
    "features = [\n",
    "    \"P_mm\",\"NDVI\",\"month_sin\",\"month_cos\",\n",
    "    \"P_mm_lag1\",\"P_mm_lag2\",\"P_mm_lag3\",\n",
    "    \"P_3mo_sum\",\"P_6mo_sum\",\n",
    "    \"NDVI_lag1\",\"NDVI_lag2\",\"NDVI_3mo_mn\"\n",
    "]\n",
    "X_tr, y_tr = train[features], train[\"Q_m3s\"]\n",
    "X_te, y_te = test[features],  test[\"Q_m3s\"]\n",
    "\n",
    "# ---------- best params from your sweep ----------\n",
    "best = dict(max_depth=6, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8)\n",
    "\n",
    "model = XGBRegressor(\n",
    "    n_estimators=1500,  # a bit larger than sweep for smoother fit\n",
    "    reg_lambda=1.0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\",\n",
    "    **best\n",
    ")\n",
    "model.fit(X_tr, y_tr)\n",
    "\n",
    "pred = model.predict(X_te)\n",
    "rmse = rmse_func(y_te, pred)\n",
    "r2   = r2_score(y_te, pred)\n",
    "nse_ = nse(y_te, pred)\n",
    "\n",
    "with open(OUT_DIR/\"model_report.txt\",\"w\") as f:\n",
    "    f.write(f\"XGB tuned — test R2 = {r2:.3f}\\n\")\n",
    "    f.write(f\"XGB tuned — test RMSE = {rmse:.3f}\\n\")\n",
    "    f.write(f\"XGB tuned — test NSE = {nse_:.3f}\\n\")\n",
    "\n",
    "print(f\"XGB tuned — R2={r2:.3f}  RMSE={rmse:.3f}  NSE={nse_:.3f}\")\n",
    "\n",
    "# ---------- example plot ----------\n",
    "sid = test[\"station_id\"].iloc[0]\n",
    "ex = test[test[\"station_id\"]==sid].copy().sort_values(\"date\")\n",
    "ex[\"pred\"] = model.predict(ex[features])\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "plt.plot(ex[\"date\"], ex[\"Q_m3s\"], label=\"Observed\")\n",
    "plt.plot(ex[\"date\"], ex[\"pred\"], label=\"Predicted\")\n",
    "plt.title(f\"Discharge — {sid} (test)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Q (m³/s)\"); plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"obs_vs_pred.png\", dpi=150); plt.close()\n",
    "\n",
    "# ---------- per-station skill ----------\n",
    "rows = []\n",
    "for sid, g in test.groupby(\"station_id\"):\n",
    "    if len(g) < 6: \n",
    "        continue\n",
    "    p = model.predict(g[features])\n",
    "    rows.append({\n",
    "        \"station_id\": sid,\n",
    "        \"R2\": r2_score(g[\"Q_m3s\"], p),\n",
    "        \"RMSE\": rmse_func(g[\"Q_m3s\"], p),\n",
    "        \"NSE\": nse(g[\"Q_m3s\"], p)\n",
    "    })\n",
    "pd.DataFrame(rows).sort_values(\"NSE\", ascending=False).to_csv(OUT_DIR/\"station_skill.csv\", index=False)\n",
    "\n",
    "# ---------- feature importance ----------\n",
    "imp = pd.Series(model.feature_importances_, index=features).sort_values(ascending=True)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "imp.plot(kind=\"barh\")\n",
    "plt.title(\"Feature importance (XGBoost — tuned)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR/\"feature_importance.png\", dpi=150); plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ac14b-9f71-4efb-afd7-d621136c5565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
